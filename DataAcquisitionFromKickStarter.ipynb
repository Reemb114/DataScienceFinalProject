{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d017bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import concurrent.futures\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49aa6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    proxies = [\n",
    "    \"ioqzyjgy:urcueqpitm55@188.68.1.50:5919\",\n",
    "    \"ioqzyjgy:urcueqpitm55@154.95.0.93:6346\",\n",
    "    \"ioqzyjgy:urcueqpitm55@103.47.52.122:8164\",\n",
    "    \"ioqzyjgy:urcueqpitm55@184.174.46.174:5803\",\n",
    "    \"ioqzyjgy:urcueqpitm55@45.41.162.165:6802\",\n",
    "    \"ioqzyjgy:urcueqpitm55@161.123.93.76:5806\",\n",
    "    \"ioqzyjgy:urcueqpitm55@107.175.119.245:6773\",\n",
    "    \"ioqzyjgy:urcueqpitm55@192.186.186.162:6204\",\n",
    "    \"ioqzyjgy:urcueqpitm55@64.137.57.35:6044\",\n",
    "    \"ioqzyjgy:urcueqpitm55@45.41.162.16:6653\"\n",
    "]\n",
    "    proxy = random.choice(proxies)\n",
    "    firefox_driver_path = \"/Users/reembeniluz/Downloads/geckodriver\"  # Replace with the actual path to the GeckoDriver executable\n",
    "    options = Options()\n",
    "    options.add_argument('-headless')\n",
    "    options.add_argument('-private')\n",
    "    options.add_argument(f\"--proxy-server={proxy}\")\n",
    "    driver = webdriver.Firefox(service=Service(firefox_driver_path), options=options)\n",
    "    print(\"driver was created\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aededcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    driver = initialize_driver()\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    links = []\n",
    "    extract_project_links(driver, links)\n",
    "    driver.quit()\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1f1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project_links(driver, links):\n",
    "    page_links = driver.execute_script(\"\"\"\n",
    "        const elements = document.querySelectorAll('div.relative.self-start a[href].block.img-placeholder.w100p');\n",
    "        const filteredLinks = [];\n",
    "        elements.forEach(element => {\n",
    "            const href = element.getAttribute('href');\n",
    "            if (!href.includes('ref=recommendation-no-result-discoverpage')) {\n",
    "                filteredLinks.push(href);\n",
    "            }\n",
    "        });\n",
    "        return filteredLinks;\n",
    "    \"\"\")\n",
    "    links.extend(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d55fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def projects_links_list_from_website(url, start_page, end_page):\n",
    "    page_seed = 2808845\n",
    "    for seed_count in range(start_page, 4):\n",
    "        page_urls = [url.replace('page=1', f'page={page}') for page in range(start_page, end_page+1)]\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "            futures = [executor.submit(scrape_page, page_url) for page_url in page_urls]\n",
    "\n",
    "            links = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                links.extend(result)\n",
    "                future.done()  # Close the thread when it finishes\n",
    "\n",
    "            url = url.replace(f'seed={page_seed}', f'seed={page_seed+1}')\n",
    "            page_seed += 1\n",
    "\n",
    "        # Move the seed_count loop inside the 'with' block\n",
    "        # Wait for all threads to finish before moving to the next iteration\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a6e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_project_data(link):\n",
    "    driver = initialize_driver()\n",
    "    driver.get(link)\n",
    "    time.sleep(5)\n",
    "    element = driver.find_element(By.XPATH, '//*[@id=\"react-project-header\"]')\n",
    "    days_left_element = driver.find_element(By.XPATH, '/html/body/main/div/div/div[1]/div/div[1]/div[1]/div[2]/div[2]/div[3]/div/div/span[1]')\n",
    "    data_initial = element.get_attribute(\"data-initial\")\n",
    "    json_data = json.loads(data_initial)\n",
    "    project_name = json_data['project']['name']\n",
    "    project_link = link\n",
    "    currency = json_data['project']['currency']\n",
    "    location = json_data['project']['location']['displayableName']\n",
    "    try:\n",
    "        parent_category = json_data['project']['category']['parentCategory']['name']\n",
    "    except:\n",
    "        parent_category = 'none'\n",
    "    category_name = json_data['project']['category']['name']\n",
    "    is_project_we_love = json_data['project']['isProjectWeLove']\n",
    "    percent_funded = json_data['project']['percentFunded']\n",
    "    goal_amount = json_data['project']['goal']['amount']\n",
    "    pledged_amount = json_data['project']['pledged']['amount']\n",
    "    duration = json_data['project']['duration']\n",
    "    description_length = len(json_data['project']['description'])\n",
    "    video_elements = driver.find_elements(By.TAG_NAME, \"video\")\n",
    "    video_count = len(video_elements)\n",
    "    image_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "    image_count = len(image_elements)\n",
    "    days_left = days_left_element.text\n",
    "\n",
    "    try:\n",
    "        riskField = driver.find_element(By.XPATH, '//*[@id=\"risks-and-challenges\"]/p')\n",
    "        riskDescLength = len(riskField.text)\n",
    "    except:\n",
    "        riskDescLength = 0\n",
    "\n",
    "    project_info = {\n",
    "        \"Project Link\":project_link,\n",
    "        \"Project Name\": project_name,\n",
    "        \"Currency\": currency,\n",
    "        \"Location\": location,\n",
    "        \"Parent Category\": parent_category,\n",
    "        \"Category Name\": category_name,\n",
    "        \"Is Project We Love\": is_project_we_love,\n",
    "        \"Percent Funded\": percent_funded,\n",
    "        \"Goal Amount\": goal_amount,\n",
    "        \"Pledged Amount\": pledged_amount,\n",
    "        \"Duration\": duration,\n",
    "        \"Description Length\": description_length,\n",
    "        \"Image Count\": image_count,\n",
    "        \"Video Count\": video_count,\n",
    "        \"Risk Desc Count\": riskDescLength,\n",
    "        \"Days Left\":days_left\n",
    "    }\n",
    "\n",
    "    driver.quit()\n",
    "    return project_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1f8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projects_data_from_links(links):\n",
    "    project_data_dic = []\n",
    "    counter = 1\n",
    "\n",
    "    def scrape_project_data_wrapper(link):\n",
    "        driver = initialize_driver()\n",
    "        try:\n",
    "            return scrape_project_data(link)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [executor.submit(scrape_project_data_wrapper, link) for link in links]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            project_data_dic.append(result)\n",
    "            print('finished with project:', counter)\n",
    "            counter += 1\n",
    "\n",
    "    return project_data_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15eddf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_table(data, filename):\n",
    "    fieldnames = [\n",
    "        \"Project Name\",\n",
    "        \"Project Link\",\n",
    "        \"Parent Category\",\n",
    "        \"Category Name\",\n",
    "        \"Location\",\n",
    "        \"Currency\",\n",
    "        \"Goal Amount\",\n",
    "        \"Pledged Amount\",\n",
    "        \"Percent Funded\",\n",
    "        \"Duration\",\n",
    "        \"Days Left\",\n",
    "        \"Description Length\",\n",
    "        \"Risk Desc Count\",\n",
    "        \"Is Project We Love\",\n",
    "        \"Image Count\",\n",
    "        \"Video Count\"\n",
    "    ]\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kickstarter.com/discover/advanced?woe_id=0&sort=magic&ref=discovery_overlay&seed=2808845&page=1\"\n",
    "links = projects_links_list_from_website(url,1,200)\n",
    "project_data_dic = projects_data_from_links(links)\n",
    "create_csv_table(project_data_dic,'sample_projects_table_data.csv')\n",
    "print('table was created successfuly')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
